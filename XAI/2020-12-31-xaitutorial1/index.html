
<!doctype html>
<html lang="en" class="no-js">
  <head>
    
      <meta charset="utf-8">
      <meta name="viewport" content="width=device-width,initial-scale=1">
      
      
      
        <link rel="canonical" href="https://simonjisu.github.io/PaperReading/XAI/2020-12-31-xaitutorial1/">
      
      <link rel="icon" href="../../assets/images/favicon.png">
      <meta name="generator" content="mkdocs-1.3.0, mkdocs-material-8.3.9">
    
    
      
        <title>Explainable Artificial Intelligence (XAI) - 1 - Soopace - Paper Reading</title>
      
    
    
      <link rel="stylesheet" href="../../assets/stylesheets/main.1d29e8d0.min.css">
      
        
        <link rel="stylesheet" href="../../assets/stylesheets/palette.cbb835fc.min.css">
        
          
          
          <meta name="theme-color" content="#4051b5">
        
      
      
    
    
    
      
        
        
        <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
        <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Roboto:300,300i,400,400i,700,700i%7CRoboto+Mono:400,400i,700,700i&display=fallback">
        <style>:root{--md-text-font:"Roboto";--md-code-font:"Roboto Mono"}</style>
      
    
    
      <link rel="stylesheet" href="../../stylesheets/extra.css">
    
    <script>__md_scope=new URL("../..",location),__md_get=(e,_=localStorage,t=__md_scope)=>JSON.parse(_.getItem(t.pathname+"."+e)),__md_set=(e,_,t=localStorage,a=__md_scope)=>{try{t.setItem(a.pathname+"."+e,JSON.stringify(_))}catch(e){}}</script>
    
      

    
    
  </head>
  
  
    
    
    
    
    
    <body dir="ltr" data-md-color-scheme="" data-md-color-primary="indigo" data-md-color-accent="">
  
    
    
    <input class="md-toggle" data-md-toggle="drawer" type="checkbox" id="__drawer" autocomplete="off">
    <input class="md-toggle" data-md-toggle="search" type="checkbox" id="__search" autocomplete="off">
    <label class="md-overlay" for="__drawer"></label>
    <div data-md-component="skip">
      
        
        <a href="#explainable-artificial-intelligence-xai-1" class="md-skip">
          Skip to content
        </a>
      
    </div>
    <div data-md-component="announce">
      
    </div>
    
    
      

<header class="md-header" data-md-component="header">
  <nav class="md-header__inner md-grid" aria-label="Header">
    <a href="../.." title="Soopace - Paper Reading" class="md-header__button md-logo" aria-label="Soopace - Paper Reading" data-md-component="logo">
      
  <img src="../../img/logo/logo.png" alt="logo">

    </a>
    <label class="md-header__button md-icon" for="__drawer">
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M3 6h18v2H3V6m0 5h18v2H3v-2m0 5h18v2H3v-2Z"/></svg>
    </label>
    <div class="md-header__title" data-md-component="header-title">
      <div class="md-header__ellipsis">
        <div class="md-header__topic">
          <span class="md-ellipsis">
            Soopace - Paper Reading
          </span>
        </div>
        <div class="md-header__topic" data-md-component="header-topic">
          <span class="md-ellipsis">
            
              Explainable Artificial Intelligence (XAI) - 1
            
          </span>
        </div>
      </div>
    </div>
    
    
    
      <label class="md-header__button md-icon" for="__search">
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.516 6.516 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5Z"/></svg>
      </label>
      <div class="md-search" data-md-component="search" role="dialog">
  <label class="md-search__overlay" for="__search"></label>
  <div class="md-search__inner" role="search">
    <form class="md-search__form" name="search">
      <input type="text" class="md-search__input" name="query" aria-label="Search" placeholder="Search" autocapitalize="off" autocorrect="off" autocomplete="off" spellcheck="false" data-md-component="search-query" required>
      <label class="md-search__icon md-icon" for="__search">
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.516 6.516 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5Z"/></svg>
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M20 11v2H8l5.5 5.5-1.42 1.42L4.16 12l7.92-7.92L13.5 5.5 8 11h12Z"/></svg>
      </label>
      <nav class="md-search__options" aria-label="Search">
        
        <button type="reset" class="md-search__icon md-icon" aria-label="Clear" tabindex="-1">
          <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M19 6.41 17.59 5 12 10.59 6.41 5 5 6.41 10.59 12 5 17.59 6.41 19 12 13.41 17.59 19 19 17.59 13.41 12 19 6.41Z"/></svg>
        </button>
      </nav>
      
    </form>
    <div class="md-search__output">
      <div class="md-search__scrollwrap" data-md-scrollfix>
        <div class="md-search-result" data-md-component="search-result">
          <div class="md-search-result__meta">
            Initializing search
          </div>
          <ol class="md-search-result__list"></ol>
        </div>
      </div>
    </div>
  </div>
</div>
    
    
      <div class="md-header__source">
        <a href="https://github.com/simonjisu/PaperReading" title="Go to repository" class="md-source" data-md-component="source">
  <div class="md-source__icon md-icon">
    
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 448 512"><!--! Font Awesome Free 6.1.1 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2022 Fonticons, Inc.--><path d="M439.55 236.05 244 40.45a28.87 28.87 0 0 0-40.81 0l-40.66 40.63 51.52 51.52c27.06-9.14 52.68 16.77 43.39 43.68l49.66 49.66c34.23-11.8 61.18 31 35.47 56.69-26.49 26.49-70.21-2.87-56-37.34L240.22 199v121.85c25.3 12.54 22.26 41.85 9.08 55a34.34 34.34 0 0 1-48.55 0c-17.57-17.6-11.07-46.91 11.25-56v-123c-20.8-8.51-24.6-30.74-18.64-45L142.57 101 8.45 235.14a28.86 28.86 0 0 0 0 40.81l195.61 195.6a28.86 28.86 0 0 0 40.8 0l194.69-194.69a28.86 28.86 0 0 0 0-40.81z"/></svg>
  </div>
  <div class="md-source__repository">
    GitHub
  </div>
</a>
      </div>
    
  </nav>
  
</header>
    
    <div class="md-container" data-md-component="container">
      
      
        
          
        
      
      <main class="md-main" data-md-component="main">
        <div class="md-main__inner md-grid">
          
            
              
              <div class="md-sidebar md-sidebar--primary" data-md-component="sidebar" data-md-type="navigation" >
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    


<nav class="md-nav md-nav--primary" aria-label="Navigation" data-md-level="0">
  <label class="md-nav__title" for="__drawer">
    <a href="../.." title="Soopace - Paper Reading" class="md-nav__button md-logo" aria-label="Soopace - Paper Reading" data-md-component="logo">
      
  <img src="../../img/logo/logo.png" alt="logo">

    </a>
    Soopace - Paper Reading
  </label>
  
    <div class="md-nav__source">
      <a href="https://github.com/simonjisu/PaperReading" title="Go to repository" class="md-source" data-md-component="source">
  <div class="md-source__icon md-icon">
    
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 448 512"><!--! Font Awesome Free 6.1.1 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2022 Fonticons, Inc.--><path d="M439.55 236.05 244 40.45a28.87 28.87 0 0 0-40.81 0l-40.66 40.63 51.52 51.52c27.06-9.14 52.68 16.77 43.39 43.68l49.66 49.66c34.23-11.8 61.18 31 35.47 56.69-26.49 26.49-70.21-2.87-56-37.34L240.22 199v121.85c25.3 12.54 22.26 41.85 9.08 55a34.34 34.34 0 0 1-48.55 0c-17.57-17.6-11.07-46.91 11.25-56v-123c-20.8-8.51-24.6-30.74-18.64-45L142.57 101 8.45 235.14a28.86 28.86 0 0 0 0 40.81l195.61 195.6a28.86 28.86 0 0 0 40.8 0l194.69-194.69a28.86 28.86 0 0 0 0-40.81z"/></svg>
  </div>
  <div class="md-source__repository">
    GitHub
  </div>
</a>
    </div>
  
  <ul class="md-nav__list" data-md-scrollfix>
    
      
      
      

  
  
  
    <li class="md-nav__item">
      <a href="../.." class="md-nav__link">
        Welcome
      </a>
    </li>
  

    
      
      
      

  
  
    
  
  
    
    <li class="md-nav__item md-nav__item--active md-nav__item--nested">
      
      
        <input class="md-nav__toggle md-toggle" data-md-toggle="__nav_2" type="checkbox" id="__nav_2" checked>
      
      
      
      
        
        
        <div class="md-nav__link md-nav__link--index ">
          <a href="../">XAI</a>
          
            <label for="__nav_2">
              <span class="md-nav__icon md-icon"></span>
            </label>
          
        </div>
      
      <nav class="md-nav" aria-label="XAI" data-md-level="1">
        <label class="md-nav__title" for="__nav_2">
          <span class="md-nav__icon md-icon"></span>
          XAI
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../2019-09-18-introxai/" class="md-nav__link">
        Explaining Explanations: An Overview of Interpretability of Machine Learning
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../2020-07-23-casm/" class="md-nav__link">
        Classifier-agnostic saliency map extraction
      </a>
    </li>
  

            
          
            
              
  
  
    
  
  
    <li class="md-nav__item md-nav__item--active">
      
      <input class="md-nav__toggle md-toggle" data-md-toggle="toc" type="checkbox" id="__toc">
      
      
        
      
      
      <a href="./" class="md-nav__link md-nav__link--active">
        Explainable Artificial Intelligence (XAI) - 1
      </a>
      
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../2021-01-14-xaitutorial2/" class="md-nav__link">
        Explainable Artificial Intelligence (XAI) - 2
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../2021-11-21-nbdt/" class="md-nav__link">
        NBDT: Neural-Backed Decision Trees
      </a>
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

    
      
      
      

  
  
  
    
    <li class="md-nav__item md-nav__item--nested">
      
      
        <input class="md-nav__toggle md-toggle" data-md-toggle="__nav_3" type="checkbox" id="__nav_3" >
      
      
      
      
        
        
        <div class="md-nav__link md-nav__link--index ">
          <a href="../../NLP/">NLP</a>
          
            <label for="__nav_3">
              <span class="md-nav__icon md-icon"></span>
            </label>
          
        </div>
      
      <nav class="md-nav" aria-label="NLP" data-md-level="1">
        <label class="md-nav__title" for="__nav_3">
          <span class="md-nav__icon md-icon"></span>
          NLP
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../NLP/2017-08-04-E2EMN/" class="md-nav__link">
        End-to-End Memory Network
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../NLP/2018-04-04-nsmcbidreclstmselfattn/" class="md-nav__link">
        A Structured Self-Attentive Sentence Embedding
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../NLP/2018-08-22-neuralnetworklm/" class="md-nav__link">
        A Neural Probabilistic Language Model
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../NLP/2020-01-14-attentionisallyouneed/" class="md-nav__link">
        Attention Is All You Need - 1
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../NLP/2020-02-02-attentionisallyouneed2/" class="md-nav__link">
        Attention Is All You Need - 2
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../NLP/2020-02-23-attentionisallyouneed3/" class="md-nav__link">
        Attention Is All You Need - 3
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../NLP/2020-07-19-maskpredict/" class="md-nav__link">
        Mask-Predict: Parallel Decoding of Conditional Masked Language Models
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../NLP/2021-08-13-hybridranking/" class="md-nav__link">
        Hybrid Ranking Network for Text-to-SQL
      </a>
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

    
      
      
      

  
  
  
    
    <li class="md-nav__item md-nav__item--nested">
      
      
        <input class="md-nav__toggle md-toggle" data-md-toggle="__nav_4" type="checkbox" id="__nav_4" >
      
      
      
      
        
        
        <div class="md-nav__link md-nav__link--index ">
          <a href="../../VISION/">VISION</a>
          
            <label for="__nav_4">
              <span class="md-nav__icon md-icon"></span>
            </label>
          
        </div>
      
      <nav class="md-nav" aria-label="VISION" data-md-level="1">
        <label class="md-nav__title" for="__nav_4">
          <span class="md-nav__icon md-icon"></span>
          VISION
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../VISION/2020-03-12-deepinsidecnn/" class="md-nav__link">
        Deep Inside Convolutional Networks: Visualising Image Classification Models and Saliency Maps
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../VISION/2021-04-20-featurevisualization/" class="md-nav__link">
        Feature Visualization
      </a>
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

    
  </ul>
</nav>
                  </div>
                </div>
              </div>
            
            
              
              <div class="md-sidebar md-sidebar--secondary" data-md-component="sidebar" data-md-type="toc" >
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    

<nav class="md-nav md-nav--secondary" aria-label="Table of contents">
  
  
  
    
  
  
</nav>
                  </div>
                </div>
              </div>
            
          
          <div class="md-content" data-md-component="content">
            <article class="md-content__inner md-typeset">
              
  
                
  <a href="https://github.com/simonjisu/PaperReading/edit/master/docs/XAI/2020-12-31-xaitutorial1.md" title="Edit this page" class="md-content__button md-icon">
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M20.71 7.04c.39-.39.39-1.04 0-1.41l-2.34-2.34c-.37-.39-1.02-.39-1.41 0l-1.84 1.83 3.75 3.75M3 17.25V21h3.75L17.81 9.93l-3.75-3.75L3 17.25Z"/></svg>
  </a>


  



<h1 id="explainable-artificial-intelligence-xai-1">Explainable Artificial Intelligence (XAI) - 1<a class="headerlink" href="#explainable-artificial-intelligence-xai-1" title="Permanent link">&para;</a></h1>
<h1 id="explainable-artificial-intelligence-xai-concepts-taxonomies-opportunities-and-challenges-toward-responsible-ai">Explainable Artificial Intelligence (XAI): Concepts, taxonomies, opportunities and challenges toward responsible AI<a class="headerlink" href="#explainable-artificial-intelligence-xai-concepts-taxonomies-opportunities-and-challenges-toward-responsible-ai" title="Permanent link">&para;</a></h1>
<p>Paper Link: <a href="https://arxiv.org/abs/1910.10045">https://arxiv.org/abs/1910.10045</a></p>
<p>XAI에 대한 전반적인 소개를 정리한 논문이 나와서 차근 차근 요약 정리해보려고 한다(무려 115페이지, reference만 6페이지). 약간의 번역 어투와 생략된 것도 있으니 영어 원문을 참고하길 바란다.</p>
<ol>
<li><a href="https://simonjisu.github.io/paper/2020/12/31/xaitutorial1.html"><span style="color:#e25252">Introduction(이번편)</span></a></li>
<li><a href="https://simonjisu.github.io/paper/2021/01/14/xaitutorial2.html">Explainability: What, why, what for and how?</a></li>
<li><a href="https://simonjisu.github.io/paper/2021/01/23/xaitutorial3.html">Transparent machine learning models</a></li>
<li>Post-hoc explainability techniques for machile learning models: Taxonomy, shallow models and deep learning</li>
<li>XAI: Opportunities, challenges and future research needs</li>
<li>Toward responsible AI: Principles of artificial intelligence, fairness, privacy and data fusion</li>
<li>Conclusions and outlook</li>
</ol>
<h1 id="1-introduction">1. Introduction<a class="headerlink" href="#1-introduction" title="Permanent link">&para;</a></h1>
<p>[expand]summary:영어원문 👈 </p>
<p>Artificial Intelligence (AI) lies at the core of many activity sectors that have embraced new information technologies [1]. While the roots of AI trace back to several decades ago, there is a clear consensus on the paramount importance featured nowadays by intelligent machines endowed with learning, reasoning and adaptation capabilities. It is by virtue of these capabilities that AI methods are achieving unprecedented levels of performance when learning to solve increasingly complex computational tasks, making them pivotal for the future development of the human society [2]. The sophistication of AI-powered systems has lately increased to such an extent that almost no human intervention is required for their design and deployment. When decisions derived from such systems ultimately affect humans’ lives (as in e.g. medicine, law or defense), there is an emerging need for understanding how such decisions are furnished by AI methods [3].</p>
<p>[/expand]</p>
<p><span style="color:#e25252">요약:</span> 인공지능이 정교해지면서 계산이 점점 복잡해지는 반면, 궁극적으로 인간의 삶에 영향을 미치는(의학, 법률, 국방) 시스템(기계)의 결정이 어떻게 내려졌는지, 우리는 이해할 필요가 있다.</p>
<p>[expand]summary:영어원문 👈 </p>
<p>While the very first AI systems were easily interpretable, the last years have witnessed the rise of opaque decision systems such as Deep Neural Networks (DNNs). The empirical success of Deep Learning (DL) models such as DNNs stems from a combination of efficient learning algorithms and their huge parametric space. The latter space comprises hundreds of layers and millions of parameters, which makes DNNs be considered as complex black-box models [4]. The opposite of black-box-ness is transparency, i.e., the search for a direct understanding of the mechanism by which a model works [5].</p>
<p>[/expand]</p>
<p><span style="color:#e25252">요약:</span> 딥러닝 모델은 효율적인 학습 알고리즘과 거대한 파라미터 공간의 결합에서 비롯된다. 그리고 black-box 모델로 간주 된다. 이의 반대는 <strong>투명성(transparency)</strong>이다.</p>
<p>[expand]summary:영어원문 👈 </p>
<p>As black-box Machine Learning (ML) models are increasingly being employed to make important predictions in critical contexts, the demand for transparency is increasing from the various stakeholders in AI [6]. The danger is on creating and using decisions that are not justifiable, legitimate, or that simply do not allow obtaining detailed explanations of their behaviour [7]. Explanations supporting the output of a model are crucial, e.g., in precision medicine, where experts require far more information from the model than a simple binary prediction for supporting their diagnosis [8]. Other examples include autonomous vehicles in transportation, security, and finance, among others.</p>
<p>[/expand]</p>
<p><span style="color:#e25252">요약:</span> Machine Learning 모델이 점점 많이 활용되면서, 이해관계자들로부터 투명성의 요구가 높아지고 있다. 예를 들어, 의료(진단), 교통(자율주행), 보안, 금융등 이 있다.</p>
<p>[expand]summary:영어원문 👈 </p>
<p>In general, humans are reticent to adopt techniques that are not directly interpretable, tractable and trustworthy [9], given the increasing demand for ethical AI [3]. It is customary to think that by focusing solely on performance, the systems will be increasingly opaque. This is true in the sense that there is a trade-off between the performance of a model and its transparency [10]. However, an improvement in the understanding of a system can lead to the correction of its deficiencies. When developing a ML model, the consideration of interpretability as an additional design driver can improve its implementability for 3 reasons:</p>
<ul>
<li>Interpretability helps ensure impartiality in decision-making, i.e. to detect, and consequently, correct from bias in the training dataset.</li>
<li>
<p>Interpretability facilitates the provision of robustness by highlighting potential adversarial perturbations that could change the prediction.</p>
</li>
<li>
<p>Interpretability can act as an insurance that only meaningful variables infer the output, i.e., guaranteeing that an underlying truthful causality exists in the model reasoning.</p>
</li>
</ul>
<p>All these means that the interpretation of the system should, in order to be considered practical, provide either an understanding of the model mechanisms and predictions, a visualization of the model’s discrimination rules, or hints on what could perturb the model [11].</p>
<p>[/expand]</p>
<p><span style="color:#e25252">요약:</span> 통상적으로 성과에만 치중할 수록 시스템은 점점 불투명해질 것이라 생각한다. 모델의 성능과 투명성 사이에 trade-off가 있다는 점은 사실이나, 모델에 대한 이해는 모델의 성능 향상을 이끌어 낼 수도 있다. 추가로 ML모델을 개발할 때, 해석 가능성을 모듈로 넣으면 세 가지 이유로 구현 가능성을 향상 시킬 수 있다.</p>
<ul>
<li>해석가능성은 의사결정에서 공정성을 보장하는데 도움이 된다. 즉, 교육 데이터 집합의 편향성을 탐지하고 결과적으로 수정한다.</li>
<li>해석가능성은 예측을 바꿀 수 있는 잠재적 적대적 섭동을 강조함으로써 건전성의 제공을 촉진한다.</li>
<li>해석가능성은 유의미한 변수만으로 산출물을 유추하는 보험으로서, 즉 모형 추론에서 근본적인 진실적 인과관계가 존재함을 보증하는 보험으로 작용할 수 있다.</li>
</ul>
<p>즉, 해석가능한 시스템은 모델 매커니즘과 예측에 대한 이해, 모델의 판결 규칙 시각화, 또는 모델을 방해하는 것에 대한 힌트 등을 제공해야한다.</p>
<p>[expand]summary:영어원문 👈 </p>
<p>In order to avoid limiting the effectiveness of the current generation of AI systems, eXplainable AI (XAI) [7] proposes creating a suite of ML techniques that 1) produce more explainable models while maintaining a high level of learning performance (e.g., prediction accuracy), and 2) enable humans to understand, appropriately trust, and effectively manage the emerging generation of artificially intelligent partners. XAI draws as well insights from the Social Sciences [12] and considers the psychology of explanation.</p>
<p>[/expand]</p>
<p><span style="color:#e25252">요약:</span> 현재의 효과적인 AI 시스템을 제한시키지 않는 선에서, eXplainable AI(XAI)은 1) 학습 퍼포먼스는 최대한으로 유지하면서 설명가능한 모델을 만들것을 제안 2) 사람이 이해하고, 적절하고 효과적으로 신뢰할 수 있도록 한다.</p>
<p>[expand]summary:영어원문 👈 </p>
<p>Fig. 1 displays the rising trend of contributions on XAI and related concepts. This literature outbreak shares its rationale with the research agendas of national governments and agencies. Although some recent surveys [8], [10], [13], [14], [15], [16], [17] summarize the upsurge of activity in XAI across sectors and disciplines, this overview aims to cover the creation of a complete unified framework of categories and concepts that allow for scrutiny and understanding of the field of XAI methods. Furthermore, we pose intriguing thoughts around the explainability of AI models in data fusion contexts with regards to data privacy and model confidentiality. This, along with other research opportunities and challenges identified throughout our study, serve as the pull factor toward Responsible Artificial Intelligence, term by which we refer to a series of AI principles to be necessarily met when deploying AI in real applications. As we will later show in detail, model explainability is among the most crucial aspects to be ensured within this methodological framework. All in all, the novel contributions of this overview can be summarized as follows:</p>
<ol>
<li>Grounded on a first elaboration of concepts and terms used in XAI-related research, we propose a novel definition of explainability that places audience (Fig. 2) as a key aspect to be considered when explaining a ML model. We also elaborate on the diverse purposes sought when using XAI techniques, from trustworthiness to privacy awareness, which round up the claimed importance of purpose and targeted audience in model explainability.</li>
<li>We define and examine the different levels of transparency that a ML model can feature by itself, as well as the diverse approaches to post-hoc explainability, namely, the explanation of ML models that are not transparent by design.</li>
<li>We thoroughly analyze the literature on XAI and related concepts published to date, covering approximately 400 contributions arranged into two different taxonomies. The first taxonomy addresses the explainability of ML models using the previously made distinction between transparency and post-hoc explainability, including models that are transparent by themselves, Deep and non-Deep (i.e., shallow) learning models. The second taxonomy deals with XAI methods suited for the explanation of Deep Learning models, using classification criteria closely linked to this family of ML methods (e.g. layerwise explanations, representation vectors, attention).</li>
<li>We enumerate a series of challenges of XAI that still remain insufficiently addressed to date. Specifically, we identify research needs around the concepts and metrics to evaluate the explainability of ML models, and outline research directions toward making Deep Learning models more understandable. We further augment the scope of our prospects toward the implications of XAI techniques in regards to confidentiality, robustness in adversarial settings, data diversity, and other areas intersecting with explainability.</li>
<li>After the previous prospective discussion, we arrive at the concept of Responsible Artificial Intelligence, a manifold concept that imposes the systematic adoption of several AI principles for AI models to be of practical use. In addition to explainability, the guidelines behind Responsible AI establish that fairness, accountability and privacy should also be considered when implementing AI models in real environments.</li>
<li>Since Responsible AI blends together model explainability and privacy/security by design, we call for a profound reflection around the benefits and risks of XAI techniques in scenarios dealing with sensitive information and/or confidential ML models. As we will later show, the regulatory push toward data privacy, quality, integrity and governance demands more efforts to assess the role of XAI in this arena. In this regard, we provide an insight on the implications of XAI in terms of privacy and security under different data fusion paradigms.</li>
</ol>
<p>[/expand]</p>
<p>{% include image.html id="119QnRBvYV4gHiuKz7kpaOVo_2b2tlhz5" desc="Fig 1. 학계에서 XAI 및 연관된 개념의 기여도 추세" width="100%" height="auto" %}</p>
<p><span style="color:#e25252">요약:</span> <code>Fig 1</code>에서 볼 수 있듯이 국가 정부 및 기관의 연구의제의 키워드 추세를 살펴보면 XAI관련 활동이 최근 급증했지만, 통일된 프레임워크가 없다. 이번 논문에서는 통일된 프레임워크의 작성하고, 개인정보 보호 및 모델 기밀성에 대해서 의견을 제시할 것이다. </p>
<ol>
<li>지금까지 XAI 관련 연구에서 사용된 개념과 용어의 기초하여, ML 모델을 설명할 때 청중(audience)을 핵심으로 고려할 것이다(그림 2). 또한 XAI 기법을 사용할 때 추구하는 다양한 목적에 따라 세분화할 것이다. 그리고 설명가능성에서 목적과 타겟 청중의 중요함을 이야기 한다.</li>
<li>다양한 레벨의 투명성을 정의하고 검토한다. 대상에는 사후(post-hoc) 설명이 가능한, 자체 설명가능한 혹은 설계에 의해 설명이 불가능한 모델들 등이 있다.</li>
<li>XAI에 관한 문헌과 지금까지 출판된 관련 개념들을 철저하게 분석하여, 대략 400개의 기여를 두 개의 다른 분류법으로 배열하였다. 첫 번째 분류법은 이전에 만든 투명성(transparency)과 사후 설명성(post-hoc explainability) 사이의 구별을 사용하여 ML 모델의 설명가능성을 다루고 있으며, 여기에는 스스로 투명하고 깊지 않은(즉, shallow 얉은) 학습 모델이 포함된다. 두 번째 분류법은 딥러닝 모델의 설명에 적합한 XAI 방법을 다루며, 이 ML 방법 계열과 밀접하게 연계된 분류 기준(예: 계층적 설명 layer-wise explanations, 표현 벡터 representation vectors, 어텐션 attention)을 사용한다.</li>
<li>지금까지도 불충분하게 다루어지지 않고 있는 XAI의 일련의 과제를 열거한다. 구체적으로는 ML 모델의 설명 가능성을 평가하기 위해 개념 및 메트릭스를 중심으로 연구 요구를 파악하고, 딥러닝 모델을 보다 이해할 수 있도록 연구 방향을 정리한다. 기밀성, 적대적 설정의 견고성, 데이터 다양성 및 설명 가능성과 교차하는 기타 영역에 관한 XAI 기법의 함축성을 향한 전망의 범위를 더욱 확대합니다.</li>
<li>앞서의 장래의 논의를 거쳐, AI 모델이 실용화하기 위해 여러 가지 AI 원리를 체계적으로 채택하는 매니폴드 개념인 책임감 있는 인공지능의 개념에 도달한다. 책임 AI를 뒷받침하는 가이드라인은 설명가능성 외에도 실제 환경에서 AI 모델을 구현할 때 공정성, 책임성, 프라이버시 등도 고려해야 한다고 규정하고 있다.</li>
<li>책임 있는 AI는 모델 설명 가능성과 개인 정보 보호/보안성을 설계별로 혼합하므로, 민감한 정보 및/또는 기밀 ML 모델을 다루는 시나리오에서 XAI 기법의 유익성과 위해성에 대해 심오한 반성을 요구한다. 나중에 보여드리겠지만, 데이터 개인 정보 보호, 품질, 무결성 및 거버넌스를 향한 규제는 이 분야에서 XAI의 역할을 평가하기 위한 더 많은 노력을 요구합니다. 이와 관련하여, 우리는 서로 다른 데이터 융합 패러다임 하에서의 프라이버시 및 보안 측면에서 XAI의 의미에 대한 통찰력을 제공한다.</li>
</ol>
<p>[expand]summary:영어원문 👈 </p>
<p>The remainder of this overview is structured as follows: first, Section 2 and subsections therein open a discussion on the terminology and concepts revolving around explainability and interpretability in AI, ending up with the aforementioned novel definition of interpretability (Section 2.1 and 2.2), and a general criterion to categorize and analyze ML models from the XAI perspective. Sections 3 and 4 proceed by reviewing recent findings on XAI for ML models (on transparent models and post-hoc techniques respectively) that comprise the main division in the aforementioned taxonomy. We also include a review on hybrid approaches among the two, to attain XAI. Benefits and caveats of the synergies among the families of methods are discussed in Section 5, where we present a prospect of general challenges and some consequences to be cautious about. Finally, Section 6 elaborates on the concept of Responsible Artificial Intelligence. Section 7 concludes the survey with an outlook aimed at engaging the community around this vibrant research area, which has the potential to impact society, in particular those sectors that have progressively embraced ML as a core technology of their activity.</p>
<p>[/expand]</p>
<p><span style="color:#e25252">요약:</span> 나머지 부분은 다음과 같이 구성되어 있다: </p>
<ul>
<li>Section 2: 설명가능성(explainability)와 해석가능성(interpretability)의 새로운 정의, XAI 관점에서 ML 모델 분류 및 분석을 위한 용어 및 개념에 대한 이야기</li>
<li>Section 3, 4: 최근 연구 결과와 하이브리드 방법</li>
<li>Section 5: 해당 방법들에 대한 장단점 및 주의해야할 몇 가지 결과들 제시</li>
<li>Section 6: "책임감 있는 인공지능" 개념에 대한 설명</li>
<li>Section 7: 사회에 영향을 미칠 가능성이 있는 연구 영역인 만큼, ML 기술을 채택한 사람들을 커뮤니티를 참여시키는 목표로 결론을 내리고자 한다.</li>
</ul>

              

  <!-- Giscus -->
  <h2 id="__comments">Comments</h2>
  <script src="https://utteranc.es/client.js"
        repo="simonjisu/utterances_comments"
        issue-term="pathname"
        theme="github-light"
        crossorigin="anonymous"
        async>
  </script>

  <!-- Reload on palette change -->
  <script>
    var palette = __md_get("__palette")
    if (palette && typeof palette.color === "object")
      if (palette.color.scheme === "slate") {
        var giscus = document.querySelector("script[src*=giscus]")
        giscus.setAttribute("data-theme", "dark") 
      }

    /* Register event handlers after documented loaded */
    document.addEventListener("DOMContentLoaded", function() {
      var ref = document.querySelector("[data-md-component=palette]")
      ref.addEventListener("change", function() {
        var palette = __md_get("__palette")
        if (palette && typeof palette.color === "object") {
          var theme = palette.color.scheme === "slate" ? "dark" : "light"

          /* Instruct Giscus to change theme */
          var frame = document.querySelector(".giscus-frame")
          frame.contentWindow.postMessage(
            { giscus: { setConfig: { theme } } },
            "https://giscus.app"
          )
        }
      })
    })
  </script>

            </article>
            
          </div>
        </div>
        
      </main>
      
        <footer class="md-footer">
  
    
    <nav class="md-footer__inner md-grid" aria-label="Footer" >
      
        
        <a href="../2020-07-23-casm/" class="md-footer__link md-footer__link--prev" aria-label="Previous: Classifier-agnostic saliency map extraction" rel="prev">
          <div class="md-footer__button md-icon">
            <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M20 11v2H8l5.5 5.5-1.42 1.42L4.16 12l7.92-7.92L13.5 5.5 8 11h12Z"/></svg>
          </div>
          <div class="md-footer__title">
            <div class="md-ellipsis">
              <span class="md-footer__direction">
                Previous
              </span>
              Classifier-agnostic saliency map extraction
            </div>
          </div>
        </a>
      
      
        
        <a href="../2021-01-14-xaitutorial2/" class="md-footer__link md-footer__link--next" aria-label="Next: Explainable Artificial Intelligence (XAI) - 2" rel="next">
          <div class="md-footer__title">
            <div class="md-ellipsis">
              <span class="md-footer__direction">
                Next
              </span>
              Explainable Artificial Intelligence (XAI) - 2
            </div>
          </div>
          <div class="md-footer__button md-icon">
            <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M4 11v2h12l-5.5 5.5 1.42 1.42L19.84 12l-7.92-7.92L10.5 5.5 16 11H4Z"/></svg>
          </div>
        </a>
      
    </nav>
  
  <div class="md-footer-meta md-typeset">
    <div class="md-footer-meta__inner md-grid">
      <div class="md-copyright">
  
  
    Made with
    <a href="https://squidfunk.github.io/mkdocs-material/" target="_blank" rel="noopener">
      Material for MkDocs
    </a>
  
</div>
      
        <div class="md-social">
  
    
    
      
      
    
    <a href="https://github.com/simonjisu" target="_blank" rel="noopener" title="github.com" class="md-social__link">
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 496 512"><!--! Font Awesome Free 6.1.1 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2022 Fonticons, Inc.--><path d="M165.9 397.4c0 2-2.3 3.6-5.2 3.6-3.3.3-5.6-1.3-5.6-3.6 0-2 2.3-3.6 5.2-3.6 3-.3 5.6 1.3 5.6 3.6zm-31.1-4.5c-.7 2 1.3 4.3 4.3 4.9 2.6 1 5.6 0 6.2-2s-1.3-4.3-4.3-5.2c-2.6-.7-5.5.3-6.2 2.3zm44.2-1.7c-2.9.7-4.9 2.6-4.6 4.9.3 2 2.9 3.3 5.9 2.6 2.9-.7 4.9-2.6 4.6-4.6-.3-1.9-3-3.2-5.9-2.9zM244.8 8C106.1 8 0 113.3 0 252c0 110.9 69.8 205.8 169.5 239.2 12.8 2.3 17.3-5.6 17.3-12.1 0-6.2-.3-40.4-.3-61.4 0 0-70 15-84.7-29.8 0 0-11.4-29.1-27.8-36.6 0 0-22.9-15.7 1.6-15.4 0 0 24.9 2 38.6 25.8 21.9 38.6 58.6 27.5 72.9 20.9 2.3-16 8.8-27.1 16-33.7-55.9-6.2-112.3-14.3-112.3-110.5 0-27.5 7.6-41.3 23.6-58.9-2.6-6.5-11.1-33.3 2.6-67.9 20.9-6.5 69 27 69 27 20-5.6 41.5-8.5 62.8-8.5s42.8 2.9 62.8 8.5c0 0 48.1-33.6 69-27 13.7 34.7 5.2 61.4 2.6 67.9 16 17.7 25.8 31.5 25.8 58.9 0 96.5-58.9 104.2-114.8 110.5 9.2 7.9 17 22.9 17 46.4 0 33.7-.3 75.4-.3 83.6 0 6.5 4.6 14.4 17.3 12.1C428.2 457.8 496 362.9 496 252 496 113.3 383.5 8 244.8 8zM97.2 352.9c-1.3 1-1 3.3.7 5.2 1.6 1.6 3.9 2.3 5.2 1 1.3-1 1-3.3-.7-5.2-1.6-1.6-3.9-2.3-5.2-1zm-10.8-8.1c-.7 1.3.3 2.9 2.3 3.9 1.6 1 3.6.7 4.3-.7.7-1.3-.3-2.9-2.3-3.9-2-.6-3.6-.3-4.3.7zm32.4 35.6c-1.6 1.3-1 4.3 1.3 6.2 2.3 2.3 5.2 2.6 6.5 1 1.3-1.3.7-4.3-1.3-6.2-2.2-2.3-5.2-2.6-6.5-1zm-11.4-14.7c-1.6 1-1.6 3.6 0 5.9 1.6 2.3 4.3 3.3 5.6 2.3 1.6-1.3 1.6-3.9 0-6.2-1.4-2.3-4-3.3-5.6-2z"/></svg>
    </a>
  
</div>
      
    </div>
  </div>
</footer>
      
    </div>
    <div class="md-dialog" data-md-component="dialog">
      <div class="md-dialog__inner md-typeset"></div>
    </div>
    <script id="__config" type="application/json">{"base": "../..", "features": ["navigation.prune", "toc.follow"], "search": "../../assets/javascripts/workers/search.b97dbffb.min.js", "translations": {"clipboard.copied": "Copied to clipboard", "clipboard.copy": "Copy to clipboard", "search.config.lang": "en", "search.config.pipeline": "trimmer, stopWordFilter", "search.config.separator": "[\\s\\-]+", "search.placeholder": "Search", "search.result.more.one": "1 more on this page", "search.result.more.other": "# more on this page", "search.result.none": "No matching documents", "search.result.one": "1 matching document", "search.result.other": "# matching documents", "search.result.placeholder": "Type to start searching", "search.result.term.missing": "Missing", "select.version.title": "Select version"}}</script>
    
    
      <script src="../../assets/javascripts/bundle.6c7ad80a.min.js"></script>
      
        <script src="../../javascripts/mathjax.js"></script>
      
        <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
      
        <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
      
    
  </body>
</html>